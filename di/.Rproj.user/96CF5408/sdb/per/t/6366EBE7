{
    "collab_server" : "",
    "contents" : "import numpy as np\nimport pandas as pd\nfrom scipy.sparse import issparse, csr_matrix, find\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.manifold.t_sne import _joint_probabilities, _joint_probabilities_nn\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial.distance import squareform\nfrom sklearn.neighbors import NearestNeighbors\n\ndef magic(data, kernel='gaussian', n_pca_components=20, random_pca=True, \n          t=6, knn=30, knn_autotune=10, epsilon=1, rescale=99, k_knn=100, perplexity=30):\n\n    if kernel not in ['gaussian']:\n        raise RuntimeError('Invalid kernel type. Must be \"gaussian\".')\n\n    #library size normalization\n    #create data_norm\n\n    #always pass in data_norm\n    if n_pca_components != None:\n        pca_projected_data = run_pca(data, n_components=n_pca_components, random=random_pca)\n    else:\n        pca_projected_data = data\n\n    if kernel == 'gaussian':\n        #run diffusion maps to get markov matrix\n        L = compute_markov(pca_projected_data, knn=knn, epsilon=epsilon, \n                                       distance_metric='euclidean', knn_autotune=knn_autotune)\n\n    #remove tsne kernel for now\n    # else:\n    #     distances = pairwise_distances(pca_projected_data, squared=True)\n    #     if k_knn > 0:\n    #         neighbors_nn = np.argsort(distances, axis=0)[:, :k_knn]\n    #         P = _joint_probabilities_nn(distances, neighbors_nn, perplexity, 1)\n    #     else:\n    #         P = _joint_probabilities(distances, perplexity, 1)\n    #     P = squareform(P)\n\n    ## QUESTION -- should this happen for gaussian kernel too??\n    #     #markov normalize P\n    #     L = np.divide(P, np.sum(P, axis=1))\n\n    #get imputed data matrix -- by default use data_norm but give user option to pick\n    new_data, L_t = impute_fast(data, L, t, rescale_percent=rescale)\n\n    return new_data\n\n\ndef run_pca(data, n_components=100, random=True):\n\n    solver = 'randomized'\n    if random != True:\n        solver = 'full'\n\n    pca = PCA(n_components=n_components, svd_solver=solver)\n    return pca.fit_transform(data)\n\n\ndef impute_fast(data, L, t, rescale_percent=0, L_t=None, tprev=None):\n\n    #convert L to full matrix\n    if issparse(L):\n        L = L.todense()\n\n    #L^t\n    print('MAGIC: L_t = L^t')\n    if L_t == None:\n        L_t = np.linalg.matrix_power(L, t)\n    else:\n        L_t = np.dot(L_t, np.linalg.matrix_power(L, t-tprev))\n\n    print('MAGIC: data_new = L_t * data')\n    data_new = np.array(np.dot(L_t, data))\n\n    #rescale data\n    if rescale_percent != 0:\n        M99 = np.percentile(data, rescale_percent, axis=0)\n        M100 = data.max(axis=0)\n        indices = np.where(M99 == 0)[0]\n        M99[indices] = M100[indices]\n        M99_new = np.percentile(data_new, rescale_percent, axis=0)\n        M100_new = data_new.max(axis=0)\n        indices = np.where(M99_new == 0)[0]\n        M99_new[indices] = M100_new[indices]\n        max_ratio = np.divide(M99, M99_new)\n        data_new = np.multiply(data_new, np.tile(max_ratio, (len(data), 1)))\n    \n    return data_new, L_t\n\n\ndef compute_markov(data, knn=10, epsilon=1, distance_metric='euclidean', knn_autotune=0):\n\n    N = data.shape[0]\n\n    # Nearest neighbors\n    nbrs = NearestNeighbors(n_neighbors=knn, metric=distance_metric).fit(data)\n    distances, indices = nbrs.kneighbors(data)\n\n    if knn_autotune > 0:\n        print('Autotuning distances')\n        for j in reversed(range(N)):\n            temp = sorted(distances[j])\n            lMaxTempIdxs = min(knn_autotune, len(temp))\n            if lMaxTempIdxs == 0 or temp[lMaxTempIdxs] == 0:\n                distances[j] = 0\n            else:\n                distances[j] = np.divide(distances[j], temp[lMaxTempIdxs])\n\n    # Adjacency matrix\n    rows = np.zeros(N * knn, dtype=np.int32)\n    cols = np.zeros(N * knn, dtype=np.int32)\n    dists = np.zeros(N * knn)\n    location = 0\n    for i in range(N):\n        inds = range(location, location + knn)\n        rows[inds] = indices[i, :]\n        cols[inds] = i\n        dists[inds] = distances[i, :]\n        location += knn\n    if epsilon > 0:\n        W = csr_matrix( (dists, (rows, cols)), shape=[N, N] )\n    else:\n        W = csr_matrix( (np.ones(dists.shape), (rows, cols)), shape=[N, N] )\n\n    # Symmetrize W\n    W = W + W.T\n\n    if epsilon > 0:\n        # Convert to affinity (with selfloops)\n        rows, cols, dists = find(W)\n        rows = np.append(rows, range(N))\n        cols = np.append(cols, range(N))\n        dists = np.append(dists/(epsilon ** 2), np.zeros(N))\n        W = csr_matrix( (np.exp(-dists), (rows, cols)), shape=[N, N] )\n\n    # Create D\n    D = np.ravel(W.sum(axis = 1))\n    D[D!=0] = 1/D[D!=0]\n\n    #markov normalization\n    T = csr_matrix((D, (range(N), range(N))), shape=[N, N]).dot(W)\n\n    return T\n\n\ndef optimal_t(data, th=0.001):\n    S = np.linalg.svd(data)\n    S = np.power(S, 2)\n    nse = np.zeros(32)\n\n    for t in range(32):\n        S_t = np.power(S, t)\n        P = np.divide(S_t, np.sum(S_t, axis=0))\n        nse[t] = np.sum(P[np.where(P > th)[0]])\n\n",
    "created" : 1488842181424.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3565365757",
    "id" : "6366EBE7",
    "lastKnownWriteTime" : 1488324600,
    "last_content_update" : 1488324600,
    "path" : "~/Dropbox (Personal)/Projects/magic/src/magic/MAGIC.py",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "python"
}