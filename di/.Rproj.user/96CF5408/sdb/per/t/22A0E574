{
    "collab_server" : "",
    "contents" : "#function to generate new cds: \nmake_cds <- function (exprs_matrix, pd, fd, expressionFamily) {\n  cds <- newCellDataSet(exprs_matrix, \n                        phenoData = new(\"AnnotatedDataFrame\", data = pd), \n                        featureData = new(\"AnnotatedDataFrame\", data = fd), \n                        expressionFamily = expressionFamily, \n                        lowerDetectionLimit = 0.1)\n  \n  if(identical(expressionFamily, tobit())) {\n    cds <- estimateSizeFactors(cds)\n    cds <- estimateDispersions(cds)\n  }\n  return(cds)\n}\n\n# function to reproduce the smooth used by Arman\n# exprs: row: cell; column: gene\nmove_avg_f <- function(exprs, window_size = 40) {\n  win_range <- nrow(exprs) - window_size\n  exprs_smooth <- exprs[-c(1:window_size), ]\n\n  res <- apply(exprs, 2, function(x) {\n    tmp <- rep(0, win_range)\n    for(i in 0:(win_range)){\n      tmp[i + 1] <- mean(x[i + c(1:window_size)])\n    }\n    return(tmp)\n  })\n  \n  return(res)\n}\n\n#function to run the ccm and save the result\ncal_cross_map <- function(ordered_exprs_mat, lib_colum, target_column, RNGseed = 2016, window_size = 20) { \n  ordered_exprs_mat <- move_avg_f(ordered_exprs_mat, window_size)\n  lib_xmap_target <- ccm(ordered_exprs_mat, E = 3, random_libs = TRUE, lib_column = lib_colum, #ENSG00000122180.4\n                         target_column = target_column, lib_sizes = seq(10, 75, by = 5), num_samples = 300, RNGseed = RNGseed)\n  target_xmap_lib <- ccm(ordered_exprs_mat, E = 3, random_libs = TRUE, lib_column = target_column, #ENSG00000122180.4\n                         target_column = lib_colum, lib_sizes = seq(10, 75, by = 5), num_samples = 300, RNGseed = RNGseed)\n  \n  lib_xmap_target_means <- ccm_means(lib_xmap_target)\n  target_xmap_lib_means <- ccm_means(target_xmap_lib)\n  \n  return(list(lib_xmap_target = lib_xmap_target, target_xmap_lib = target_xmap_lib, \n              lib_xmap_target_means = lib_xmap_target_means, target_xmap_lib_means = target_xmap_lib_means,\n              mean_lib_xmap_target_means = mean(lib_xmap_target_means$rho[is.finite(lib_xmap_target_means$rho)], na.rm = T), \n              mean_target_xmap_lib_means = mean(target_xmap_lib_means$rho[is.finite(target_xmap_lib_means$rho)], na.rm = T)))\n}\n\n#function to plot the result from ccm\nplot_cross_map <- function(lib_xmap_target_means, target_xmap_lib_means, lib_name, target_name){\n  legend_names <- c(paste(lib_name, 'xmap', target_name), paste(target_name, 'xmap', lib_name))\n  \n  xmap_all <- rbind(lib_xmap_target_means, target_xmap_lib_means)\n  xmap_all$type <- c(rep('a_xmap_t_means', nrow(lib_xmap_target_means)), rep('t_xmap_a_means', nrow(target_xmap_lib_means)))\n  y_max <- max(xmap_all$rho, na.rm = T) + 0.1\n  \n  lib_rng <- range(xmap_all$lib_size)\n  p1 <- ggplot(aes(lib_size, pmax(0, rho)), data = xmap_all) + geom_line(aes(color = type)) + xlim(lib_rng) + \n    xlab(\"Library Size\") + ylab(\"Cross Map Skill (rho)\") + scale_color_discrete(labels=legend_names) + \n    scale_x_discrete(breaks = unique(xmap_all$lib_size)) + monocle_theme_opts()\n  \n  return(p1)\n}\n\n#parallel the CCM algorithm: \nparallelCCM <- function(ordered_exprs_mat, cores = detectCores() / 2, window_size = 20) {\n  ordered_exprs_mat <- ordered_exprs_mat\n  combn_mat <- combn(1:ncol(ordered_exprs_mat), 2)\n  \n  combn_mat_split <- split(t(combn_mat), 1:ncol(combn_mat))\n  CCM_res <- mclapply(combn_mat_split, function(x, ordered_exprs_mat){ \n    col_names <- colnames(ordered_exprs_mat)[x]\n    cross_map_res <- cal_cross_map(ordered_exprs_mat[, col_names], col_names[1], col_names[2])\n    cross_map_res[c('mean_lib_xmap_target_means', 'mean_target_xmap_lib_means')]\n  }, ordered_exprs_mat = ordered_exprs_mat, mc.cores = cores)\n  \n  return(list(CCM_res = CCM_res, combn_mat_split = combn_mat_split, gene_names = colnames(ordered_exprs_mat)))\n}\n\n#function to prepare the result for CCM: \nprepare_ccm_res <- function(parallel_res, gene_names = NULL){\n  if(is.null(gene_names))\n    gene_names <- parallel_res$gene_names\n  parallel_res_list <- lapply(1:length(parallel_res$CCM_res), function(x, gene_names) {\n  lib_xmap_target_means <- parallel_res$CCM_res[[x]]$mean_lib_xmap_target_means #mean of mean under different library\n  target_xmap_lib_means <- parallel_res$CCM_res[[x]]$mean_target_xmap_lib_means #mean of mean under different library\n  lib_name <- parallel_res$combn_mat_split[[x]][1] \n  target_name <- parallel_res$combn_mat_split[[x]][2] \n  data.frame(lib_name = c(gene_names[lib_name], gene_names[target_name]),\n             target_name = c(gene_names[target_name], gene_names[lib_name]),\n             mean_rho = c(lib_xmap_target_means, target_xmap_lib_means))\n  }, as.character(gene_names))\n  \n  parallel_res_df <- do.call(rbind.data.frame, parallel_res_list)\n  parallel_res_mat <- dcast(parallel_res_df, lib_name ~ target_name, fun.aggregate=mean)\n  \n  row.names(parallel_res_mat) <- parallel_res_mat$lib_name\n  parallel_res_mat <- parallel_res_mat[, -1]\n  \n  parallel_res_mat[!is.finite(as.matrix(parallel_res_mat))] <- 0\n  diag(parallel_res_mat) <- 0\n  \n  return(parallel_res_mat)\n}\n\n#output format: Gene_1_ID\tGene_1_NAME\tGene_2_ID\tGene_2_NAME\tdelay_max\tRDI\nparallel_cal_grangertest <- function(exprs_data, cores =  detectCores() - 2, delays = 1, smoothing = T, filename = 'granger_res.txt') {\n  gene_name <- colnames(exprs_data)\n  combn_df <- combn(gene_name, m = 2)\n  split_combn_df <- split(t(combn_df), rep(1:ncol(combn_df), nrow(combn_df)))\n  \n  mclapply(split_combn_df, function(x, fname, dl, smooth = smoothing) {\n    subset_df <- exprs_data[, c(x)]\n    if(smooth){\n      subset_df <- move_avg_f(subset_df, window_size = 10)\n      subset_df <- as.data.frame(subset_df)\n    }\n    colnames(subset_df) <- c('x0', 'x1')\n    \n    res_df <- tryCatch({\n      res <- cal_grangertest(subset_df, delays = dl)\n      res\n    }, error = function(e){\n      print(e)\n      res <- data.frame(x0_by_x1 = -100,  x1_by_x0 = -100) #change this to -100 for cases where we throw numerical errors \n      \n      res\n    })\n\n    df <- data.frame(Gene_1_ID = c(x[1], x[2]), Gene_1_NAME = c(x[1], x[2]), Gene_2_ID = c(x[2], x[1]), Gene_2_NAME = c(x[2], x[1]), delay_max = NA, \n                     granger = c(mean(res_df$x1_by_x0), mean(res_df$x0_by_x1)) )\n    write.table(file = fname, df, append = T, row.names = F, col.names = F, quote = F)\n  }, mc.cores = cores, fname = filename, dl = delays, smooth = smoothing)\n}\n\n#functions to perform granger tests using lmtest package: \n# add new options to select a range of delays: \ncal_grangertest <- function(ordered_exprs_mat, delays = 1) {\n  df <- data.frame(ordered_exprs_mat)\n  x0_by_x1 <- rep(0, length(delays))\n  x1_by_x0 <- rep(0, length(delays))\n  \n  for(i in 1:length(delays)) {\n    \n    x0_by_x1[i] <- grangertest(x0 ~ x1, order = delays[i], data = df)$`Pr(>F)`[2]\n    x1_by_x0[i] <- grangertest(x1 ~ x0, order = delays[i], data = df)$`Pr(>F)`[2]    \n  }\n\n  return(data.frame(x0_by_x1 = max(x0_by_x1), \n                    x1_by_x0 = max(x1_by_x0)))\n}\n\n#functions to perform granger tests using VAR package: \ncal_grangertest_var <- function(ordered_exprs_mat, order = 1) {\n  var <- VAR(ordered_exprs_mat, p = order, type = \"const\")\n  x1_by_x0 <- causality(var, cause = \"x0\")$Granger\n  x0_by_x1 <- causality(var, cause = \"x1\")$Granger\n  \n  return(data.frame(x0_by_x1 = x0_by_x1$p.value, \n                    x1_by_x0 = x1_by_x0$p.value))\n}\n\n#function to implement the so called MAGIC method: \ndiffusion_maps <- function (data, bw_ini = 0, pseudo_cnt = 1, neighbours = 0.2,\n          log2_data = F, max_components = 3) {\n  if (log2_data)\n    data <- t(log2(data + pseudo_cnt))\n  else data <- t(data)\n  data_dm_bw_res <- diffusion_maps_bw(data, pseudo_cnt = pseudo_cnt,\n                                      neighbours = neighbours, bw_ini = 0, iter = 100, step = 0.02,\n                                      log2_data = log2_data)\n  bw = 10^(0.2 * (which(data_dm_bw_res$av_d_sigma == max(data_dm_bw_res$av_d_sigma)) +\n                    1))\n  nn <- ceiling(nrow(data) * neighbours)\n  d2 <- as.matrix(dist(data)^2)\n  sigma <- bw^2\n  W <- exp(-d2/(2 * sigma))\n  R <- apply(d2, 2, function(x) sort(x)[nn])\n  R <- matrix(rep(R, ncol(d2)), ncol = ncol(d2))\n  W <- (d2 < R) * W\n  W <- W + t(W)\n  D <- colSums(W, na.rm = T)\n  q <- D %*% t(D)\n  diag(W) <- 0\n  H <- W/q\n  colS <- colSums(H)\n  Hp <- t(t(H)/colS)\n  E <- eigen(Hp)\n  eigOrd <- order(Re(E$values), decreasing = TRUE)\n  E$values <- E$values[eigOrd][-1]\n  E$vectors <- E$vectors[, eigOrd][, -1]\n  rownames(E$vectors) <- rownames(data)\n  colnames(E$vectors) <- 1:ncol(E$vectors)\n  diffMap <- t(E$vectors)\n  return(t(diffMap[1:max_components, ]))\n}\n\nget_ka_dist <- function(X, K = 5) {\n  N <- ncol(X)\n  norm_sq <- repmat(t(colSums(X^2)), N, 1)\n  dist_sq <- norm_sq + t(norm_sq) - 2 * t(X) %*% X\n  sort_idx <- t(apply(dist_sq, 2, function(x) sort(x, index.return = T)$ix ))\n  knn_idx <- sort_idx[, c(1, K + 1)]\n  \n  distance <- apply(knn_idx, 1, function(ind_vec) sqrt(sum((X[, ind_vec[1]] - X[, ind_vec[2]])^2)) )\n\n  return(distance)\n}\n\n#implement based on DPT\n# D: row: cell; column genes\nlibrary(RANN)\nlibrary(expm) \nMAGIC_R <- function(D, kernel='gaussian', n_pca_components=2, random_pca=True, \n           t=6, knn=30, knn_autotune=10, epsilon=1, rescale=99, k_knn=100, perplexity=30,\n           var_explained = 0.75) {\n    #library size normalization\n    Libsize <- rowSums(D)\n    D_norm <- D / Libsize * median(Libsize)\n    \n    #PCA\n    res <- prcomp(D_norm, center = T, scale = F)\n    \n    #select only the top \n    std_dev <- res$sdev \n    pr_var <- std_dev^2\n    prop_varex <- pr_var/sum(pr_var)\n    \n    D_pca <- res$x[, 1:max(n_pca_components, min(which(cumsum(prop_varex) >= var_explained)))]\n    \n    # #𝐷𝑖𝑠𝑡 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒_𝑚𝑎𝑡𝑟𝑖𝑥(𝐷)\n    # Dist <- as.matrix(dist(D_pca))\n    # # 𝜎 𝑖 ￼ = 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑖, 𝑛𝑒𝑖𝑔h𝑏𝑜𝑟 𝑖, 𝑘𝑎 )\n    # sigma <- get_ka_dist(X = t(D_pca), K = ka)\n    # #𝐴 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑎𝑓𝑓𝑖𝑛𝑖𝑡𝑦_𝑚𝑎𝑡𝑟𝑖𝑥(𝐷𝑖𝑠𝑡)\n    # A <- exp(- (Dist / sigma)^2 )\n    # A <- A + t(A)\n    # #𝑀 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑚𝑎𝑟𝑘𝑜𝑣_𝑎𝑓𝑓𝑖𝑛𝑖𝑡𝑦_𝑚𝑎𝑡𝑟𝑖𝑥(𝐴)\n    # M <- A / rowSums(A)\n    \n    L <- compute_markov(D_pca, knn=knn, epsilon=epsilon, \n                        distance_metric='euclidean', knn_autotune=knn_autotune)\n    \n    #D_imputed = M^t * D\n    D_imputed <- as.matrix(L) %^% t %*% D\n    #D_rescaled = Rescale(D_imputed)\n    D_rescaled <- t(t(D_imputed) * (apply(D, 2, function(x) quantile(x, 0.99)) ) / rowMax(t(D_imputed)))\n    #Dimputed = D_rescaled\n    return(D_rescaled)\n    }\n\n# update the section on calculating the Markov matrix\ncompute_markov <- function(data, knn = 10, epsilon = 1, distance_metric = 'euclidean', knn_autotune = 0) {\n  N <- nrow(data)\n  #Nearest neighbors\n  nbrs <- RANN::nn2(data, k = knn)\n  distances <- nbrs$nn.dists\n  indices =  nbrs$nn.idx\n  if(knn_autotune > 0) {\n    print('Autotuning distance')\n    \n    for(j in rev(1:N)) {\n      temp <- sort(distances[j, ])\n      lMaxTempIdxs = min(knn_autotune + 1, length(temp))\n      if(lMaxTempIdxs == 1 | temp[lMaxTempIdxs] == 0)\n        distances[j, ] <- 0\n      else\n        distances[j, ] <- distances[j, ] / temp[lMaxTempIdxs]\n    }\n  }\n  \n  rows <- rep(0, N * knn)\n  cols <- rep(0, N * knn)\n  dists <- rep(0, N * knn)\n  location <- 1\n  \n  for(i in 1:N) {\n    inds <- location:(location + knn - 1)\n    rows[inds] <- indices[i, ]\n    cols[inds] <- i\n    dists[inds] <- distances[i, ]\n    location <- location + knn\n  }\n  \n  if(epsilon > 0)\n    W <- sparseMatrix(rows, cols, x = dists, dims = c(N, N))\n  else\n    W <- sparseMatrix(rows, cols, x = rep(1, nrow(dist) * ncol(dist)), dims = c(N, N))\n  \n  #Symmetrize W\n  W <- W + t(W)\n  if(epsilon > 0){\n    # Convert to affinity (with selfloops)\n    tmp <- which(as.matrix(W) > 0, arr.ind = T)\n    rows <- tmp[, 1]\n    cols <- tmp[, 2]\n    dists <- W[tmp]\n    rows <- c(rows, 1:N)\n    cols <- c(cols, 1:N)\n    dists <- c(dists/(epsilon^2), rep(0, N))\n    W <- sparseMatrix(rows, cols, x = exp(-dists), dims = c(N, N))\n  }\n  \n  # Create D\n  D <- rowSums(W)\n  D[D != 0] <- 1 / (D[D != 0])\n  \n  #markov normalization\n  T <- sparseMatrix(1:N, 1:N, x = D, dims = c(N, N)) %*% W\n  return(T)\n}\n\n#   function(D, kernel='gaussian', n_pca_components=20, random_pca=True, \n#                     t=6, knn=30, knn_autotune=10, epsilon=1, rescale=99, k_knn=100, perplexity=30,\n#                     var_explained = 0.75) {\n#   #library size normalization\n#   Libsize <- rowSums(D)\n#   D_norm <- D / Libsize * median(Libsize)\n# \n#   #PCA\n#   res <- prcomp(D_norm, center = T, scale = F)\n#   \n#   #select only the top \n#   std_dev <- res$sdev \n#   pr_var <- std_dev^2\n#   prop_varex <- pr_var/sum(pr_var)\n#   \n#   D_pca <- res$x[, 1:max(n_pca_components, min(which(cumsum(prop_varex) >= var_explained)))]\n#   \n#   # #𝐷𝑖𝑠𝑡 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒_𝑚𝑎𝑡𝑟𝑖𝑥(𝐷)\n#   # Dist <- as.matrix(dist(D_pca))\n#   # # 𝜎 𝑖 ￼ = 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑖, 𝑛𝑒𝑖𝑔h𝑏𝑜𝑟 𝑖, 𝑘𝑎 )\n#   # sigma <- get_ka_dist(X = t(D_pca), K = ka)\n#   # #𝐴 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑎𝑓𝑓𝑖𝑛𝑖𝑡𝑦_𝑚𝑎𝑡𝑟𝑖𝑥(𝐷𝑖𝑠𝑡)\n#   # A <- exp(- (Dist / sigma)^2 )\n#   # A <- A + t(A)\n#   # #𝑀 = 𝑐𝑜𝑚𝑝𝑢𝑡𝑒_𝑚𝑎𝑟𝑘𝑜𝑣_𝑎𝑓𝑓𝑖𝑛𝑖𝑡𝑦_𝑚𝑎𝑡𝑟𝑖𝑥(𝐴)\n#   # M <- A / rowSums(A)\n#   \n#   L <- compute_markov(D_pca, knn=knn, epsilon=epsilon, \n#                  distance_metric='euclidean', knn_autotune=knn_autotune)\n#   \n#   #D_imputed = M^t * D\n#   D_imputed <- L^t %*% D\n#   #D_rescaled = Rescale(D_imputed)\n#   D_rescaled <- D_imputed * (apply(D, 1, function(x) as.matrix(t)quanile(x, 0.99)) ) / rowMax(D_imputed)\n#   \n#   #Dimputed = D_rescaled\n#   return(D_rescaled)\n# }\n\n",
    "created" : 1488842201446.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "471022261",
    "id" : "22A0E574",
    "lastKnownWriteTime" : 1488418610,
    "last_content_update" : 1488418610,
    "path" : "~/Dropbox (Personal)/Projects/Causal_network/causal_network/Scripts/function.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}